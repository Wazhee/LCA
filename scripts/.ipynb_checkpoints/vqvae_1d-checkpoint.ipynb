{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a68cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf4b10",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efaa52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- required for med-clip ----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#---- required for dataloader ----\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "#---- required for linear SVM ----\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "#---- Required for t-SNE ----\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac7994cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define age group assignment function\n",
    "def assign_age_group(age):\n",
    "    if 55 <= age < 60:\n",
    "        return '55-60'\n",
    "    elif 60 <= age < 65:\n",
    "        return '60-65'\n",
    "    elif 65 <= age < 70:\n",
    "        return '65-70'\n",
    "    elif age >= 70:\n",
    "        return '70+'\n",
    "    else:\n",
    "        return 'Under 50'  \n",
    "\n",
    "def get_patient_data():\n",
    "    #---- Load training embeddings ----\n",
    "    train_data = np.load('nlst_train_with_labels.npz', allow_pickle=True)[\"arr_0\"].item()\n",
    "    test_data = np.load('nlst_tune_with_labels.npz', allow_pickle=True)[\"arr_0\"].item()\n",
    "    # construct dataframes\n",
    "    train_df = pd.DataFrame.from_dict(train_data, orient='index')\n",
    "    test_df = pd.DataFrame.from_dict(test_data, orient='index')\n",
    "    # Acquire unique identifiers\n",
    "    train_df[\"pid\"] = [k.split('/')[1] for k in list(train_data.keys())]\n",
    "    test_df[\"pid\"] = [k.split('/')[1] for k in list(test_data.keys())]\n",
    "    # Replace first row with indices\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #---- Load patient demographics ----\n",
    "    df = pd.read_csv(\"nlst_780_prsn_idc_20210527.csv\")\n",
    "    df[\"gender\"] = df[\"gender\"].map({1:\"M\", 2:\"F\"})\n",
    "\n",
    "    #---- add patient demographics to dataset ---- \n",
    "    train_df['pid'], test_df['pid'], df['pid'] = train_df['pid'].astype(str), test_df['pid'].astype(str), df['pid'].astype(str)\n",
    "    train_df = train_df.merge(df[['pid', 'gender', \"age\", \"race\", \"can_scr\"]], on='pid', how='left')\n",
    "    test_df = test_df.merge(df[['pid', 'gender', \"age\", \"race\", \"can_scr\"]], on='pid', how='left')\n",
    "    # define age groups\n",
    "    train_df['Age_group'], test_df['Age_group'] = train_df['age'].apply(assign_age_group), test_df['age'].apply(assign_age_group)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70643d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18767/3017132832.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  demo_df[\"gender\"] = demo_df[\"gender\"].map({1:\"M\", 2:\"F\"})\n",
      "/tmp/ipykernel_18767/3017132832.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['pid'], test_df['pid'], demo_df['pid'] = train_df['pid'].astype(str), test_df['pid'].astype(str), demo_df['pid'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "#---- Load training embeddings ----\n",
    "train_data = np.load('nlst_train_with_labels.npz', allow_pickle=True)[\"arr_0\"].item()\n",
    "test_data = np.load('nlst_tune_with_labels.npz', allow_pickle=True)[\"arr_0\"].item()\n",
    "# construct dataframes\n",
    "train_df = pd.DataFrame.from_dict(train_data, orient='index')\n",
    "test_df = pd.DataFrame.from_dict(test_data, orient='index')\n",
    "# Acquire unique identifiers\n",
    "train_df[\"pid\"] = [k.split('/')[1] for k in list(train_data.keys())]\n",
    "test_df[\"pid\"] = [k.split('/')[1] for k in list(test_data.keys())]\n",
    "# Replace first row with indices\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#---- Load patient demographics ----\n",
    "df = pd.read_csv(\"nlst_780_prsn_idc_20210527.csv\")\n",
    "demo_df = df[[\"pid\",\"race\", \"gender\", \"age\", \"can_scr\"]]\n",
    "demo_df[\"gender\"] = demo_df[\"gender\"].map({1:\"M\", 2:\"F\"})\n",
    "\n",
    "#---- add patient demographics to dataset ---- \n",
    "train_df['pid'], test_df['pid'], demo_df['pid'] = train_df['pid'].astype(str), test_df['pid'].astype(str), demo_df['pid'].astype(str)\n",
    "train_df = train_df.merge(demo_df[['pid', 'gender', \"age\", \"race\", \"can_scr\"]], on='pid', how='left')\n",
    "test_df = test_df.merge(demo_df[['pid', 'gender', \"age\", \"race\", \"can_scr\"]], on='pid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c614a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_datasets(x_train, x_test):\n",
    "    \"\"\"\n",
    "    Standard Scale test and train data\n",
    "    \"\"\"\n",
    "    standard_scaler = MinMaxScaler()\n",
    "    x_train_scaled = standard_scaler.fit_transform(x_train)\n",
    "    x_test_scaled = standard_scaler.transform(x_test)\n",
    "    return x_train_scaled, x_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf72c549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (52696, 1408) y shape:  (52696,)\n",
      "torch.Size([32, 1408])\n"
     ]
    }
   ],
   "source": [
    "#---- Load train/ test dataset ----\n",
    "X_train = np.vstack(list(train_df[\"embedding\"]))\n",
    "X_test = np.array(list(test_df[\"embedding\"]))\n",
    "y_train, y_test = train_df[\"cancer_in_2\"].values,  test_df[\"cancer_in_2\"].values\n",
    "\n",
    "x_train_scaled, x_test_scaled = scale_datasets(X_train, X_test)\n",
    "print(\"X shape: \", x_train_scaled.shape, \"y shape: \", y_train.shape)\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Dataset\n",
    "train_dataset = TensorDataset(x_train_tensor, x_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, x_test_tensor)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check shape\n",
    "for (batch_idx, train_tensors) in enumerate(train_loader):\n",
    "    print(train_tensors[0].shape)  # --> torch.Size([32, 1408])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91990908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f2f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c14e688",
   "metadata": {},
   "source": [
    "## Define VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc3ca94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/airalcorn2/vqvae-pytorch/master/vqvae.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28a57753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from vqvae1D import VQVAE\n",
    "\n",
    "torch.set_printoptions(linewidth=160)\n",
    "\n",
    "\n",
    "def display_img_tensors_as_grid(img_tensors, nrows, f):\n",
    "    img_tensors = img_tensors.permute(0, 2, 3, 1)\n",
    "    imgs_array = img_tensors.detach().cpu().numpy()\n",
    "    imgs_array[imgs_array < -0.5] = -0.5\n",
    "    imgs_array[imgs_array > 0.5] = 0.5\n",
    "    imgs_array = 255 * (imgs_array + 0.5)\n",
    "    (batch_size, img_size) = img_tensors.shape[:2]\n",
    "    ncols = batch_size // nrows\n",
    "    img_arr = np.zeros((nrows * batch_size, ncols * batch_size, 3))\n",
    "    for idx in range(batch_size):\n",
    "        row_idx = idx // ncols\n",
    "        col_idx = idx % ncols\n",
    "        row_start = row_idx * img_size\n",
    "        row_end = row_start + img_size\n",
    "        col_start = col_idx * img_size\n",
    "        col_end = col_start + img_size\n",
    "        img_arr[row_start:row_end, col_start:col_end] = imgs_array[idx]\n",
    "\n",
    "    display(Image.fromarray(img_arr.astype(np.uint8), \"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d9f5e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQVAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv): Sequential(\n",
       "      (down0): Conv1d(1, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "      (relu0): ReLU()\n",
       "      (down1): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "      (relu1): ReLU()\n",
       "      (final_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (residual_stack): ResidualStack(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (2): ReLU()\n",
       "          (3): Conv1d(32, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_vq_conv): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "  (vq): VectorQuantizer(\n",
       "    (N_i_ts): SonnetExponentialMovingAverage()\n",
       "    (m_i_ts): SonnetExponentialMovingAverage()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (residual_stack): ResidualStack(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (2): ReLU()\n",
       "          (3): Conv1d(32, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (upconv): Sequential(\n",
       "      (up0): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "      (relu0): ReLU()\n",
       "      (up1): ConvTranspose1d(64, 3, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model.\n",
    "device = torch.device(\"cuda:0\")\n",
    "use_ema = True\n",
    "model_args = {\n",
    "    \"in_channels\": 1,\n",
    "    \"num_hiddens\": 128,\n",
    "    \"num_downsampling_layers\": 2,\n",
    "    \"num_residual_layers\": 2,\n",
    "    \"num_residual_hiddens\": 32,\n",
    "    \"embedding_dim\": 64,\n",
    "    \"num_embeddings\": 512,\n",
    "    \"use_ema\": use_ema,\n",
    "    \"decay\": 0.99,\n",
    "    \"epsilon\": 1e-5,\n",
    "}\n",
    "model = VQVAE(**model_args).to(device)\n",
    "\n",
    "# Initialize dataset.\n",
    "batch_size = 16\n",
    "workers = 10\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[1.0, 1.0, 1.0])\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "data_root = \"data\"\n",
    "# train_dataset = CIFAR10(data_root, True, transform, download=True)\n",
    "train_data_variance = np.var(x_train_scaled / 255)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    ")\n",
    "\n",
    "# Multiplier for commitment loss. See Equation (3) in \"Neural Discrete Representation\n",
    "# Learning\".\n",
    "beta = 0.25\n",
    "\n",
    "# Initialize optimizer.\n",
    "train_params = [params for params in model.parameters()]\n",
    "lr = 3e-4\n",
    "optimizer = optim.Adam(train_params, lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train model.\n",
    "epochs = 50\n",
    "eval_every = 200\n",
    "best_train_loss = float(\"inf\")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6dbecda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "batch_idx: 500\n",
      "total_train_loss: 348782.023609375\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 348205.982703125\n",
      "\n",
      "epoch: 0\n",
      "batch_idx: 1000\n",
      "total_train_loss: 5109493.7503125\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 5049845.370875\n",
      "\n",
      "epoch: 0\n",
      "batch_idx: 1500\n",
      "total_train_loss: 25870891.891\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 25482115.182\n",
      "\n",
      "epoch: 0\n",
      "batch_idx: 2000\n",
      "total_train_loss: 76133516.328\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 74802314.024\n",
      "\n",
      "epoch: 0\n",
      "batch_idx: 2500\n",
      "total_train_loss: 222020987.712\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 217136639.2\n",
      "\n",
      "epoch: 0\n",
      "batch_idx: 3000\n",
      "total_train_loss: 365490919.936\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 349068052.448\n",
      "\n",
      "epoch: 1\n",
      "batch_idx: 500\n",
      "total_train_loss: 1596087290.88\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1524191638.016\n",
      "\n",
      "epoch: 1\n",
      "batch_idx: 1000\n",
      "total_train_loss: 2842690000.64\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2720599952.896\n",
      "\n",
      "epoch: 1\n",
      "batch_idx: 1500\n",
      "total_train_loss: 4481104120.32\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 4303395090.944\n",
      "\n",
      "epoch: 1\n",
      "batch_idx: 2000\n",
      "total_train_loss: 5594079017.984\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 5322749748.224\n",
      "\n",
      "epoch: 1\n",
      "batch_idx: 2500\n",
      "total_train_loss: 10155514912.768\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 9528487196.672\n",
      "\n",
      "epoch: 1\n",
      "batch_idx: 3000\n",
      "total_train_loss: 14693032046.592\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 13674363168.768\n",
      "\n",
      "epoch: 2\n",
      "batch_idx: 500\n",
      "total_train_loss: 38636567887.872\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 35235161528.32\n",
      "\n",
      "epoch: 2\n",
      "batch_idx: 1000\n",
      "total_train_loss: 47483340910.592\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 41945737302.016\n",
      "\n",
      "epoch: 2\n",
      "batch_idx: 1500\n",
      "total_train_loss: 55888700047.36\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 48771916324.864\n",
      "\n",
      "epoch: 2\n",
      "batch_idx: 2000\n",
      "total_train_loss: 72648496791.552\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 63599438725.12\n",
      "\n",
      "epoch: 2\n",
      "batch_idx: 2500\n",
      "total_train_loss: 102236231139.328\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 91074924085.248\n",
      "\n",
      "epoch: 2\n",
      "batch_idx: 3000\n",
      "total_train_loss: 121336637685.76\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 105486087946.24\n",
      "\n",
      "epoch: 3\n",
      "batch_idx: 500\n",
      "total_train_loss: 261483204116.48\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 224124908896.256\n",
      "\n",
      "epoch: 3\n",
      "batch_idx: 1000\n",
      "total_train_loss: 371902215258.112\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 321776970760.192\n",
      "\n",
      "epoch: 3\n",
      "batch_idx: 1500\n",
      "total_train_loss: 430990946697.216\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 362104634736.64\n",
      "\n",
      "epoch: 3\n",
      "batch_idx: 2000\n",
      "total_train_loss: 540656915775.488\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 440925123641.344\n",
      "\n",
      "epoch: 3\n",
      "batch_idx: 2500\n",
      "total_train_loss: 815235687579.648\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 692216172773.376\n",
      "\n",
      "epoch: 3\n",
      "batch_idx: 3000\n",
      "total_train_loss: 928368407216.128\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 787466246815.744\n",
      "\n",
      "epoch: 4\n",
      "batch_idx: 500\n",
      "total_train_loss: 1365994648764.416\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1108092435169.28\n",
      "\n",
      "epoch: 4\n",
      "batch_idx: 1000\n",
      "total_train_loss: 1744977207754.752\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1392027154251.776\n",
      "\n",
      "epoch: 4\n",
      "batch_idx: 1500\n",
      "total_train_loss: 2404290278457.344\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1936388393533.44\n",
      "\n",
      "epoch: 4\n",
      "batch_idx: 2000\n",
      "total_train_loss: 3508517399429.12\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2931343586492.416\n",
      "\n",
      "epoch: 4\n",
      "batch_idx: 2500\n",
      "total_train_loss: 3477539545153.536\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2810872864178.176\n",
      "\n",
      "epoch: 4\n",
      "batch_idx: 3000\n",
      "total_train_loss: 3548078553956.352\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2750324985036.8\n",
      "\n",
      "epoch: 5\n",
      "batch_idx: 500\n",
      "total_train_loss: 4078051886366.72\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2943766831824.896\n",
      "\n",
      "epoch: 5\n",
      "batch_idx: 1000\n",
      "total_train_loss: 5142024921022.464\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3787944187133.952\n",
      "\n",
      "epoch: 5\n",
      "batch_idx: 1500\n",
      "total_train_loss: 6676164555833.344\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 5179857862393.856\n",
      "\n",
      "epoch: 5\n",
      "batch_idx: 2000\n",
      "total_train_loss: 8891831220699.137\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 7321048039030.784\n",
      "\n",
      "epoch: 5\n",
      "batch_idx: 2500\n",
      "total_train_loss: 10356654421835.775\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 8672628682260.48\n",
      "\n",
      "epoch: 5\n",
      "batch_idx: 3000\n",
      "total_train_loss: 10738981270454.271\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 8706024995291.136\n",
      "\n",
      "epoch: 6\n",
      "batch_idx: 500\n",
      "total_train_loss: 12382646936010.752\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 10148581644173.312\n",
      "\n",
      "epoch: 6\n",
      "batch_idx: 1000\n",
      "total_train_loss: 10969419473747.969\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 8646077305585.664\n",
      "\n",
      "epoch: 6\n",
      "batch_idx: 1500\n",
      "total_train_loss: 11501755036598.271\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 9125321641033.729\n",
      "\n",
      "epoch: 6\n",
      "batch_idx: 2000\n",
      "total_train_loss: 12753897212870.656\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 9912492614483.969\n",
      "\n",
      "epoch: 6\n",
      "batch_idx: 2500\n",
      "total_train_loss: 14784492315082.752\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 11584123189592.064\n",
      "\n",
      "epoch: 6\n",
      "batch_idx: 3000\n",
      "total_train_loss: 15487485876371.455\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 11900396386648.064\n",
      "\n",
      "epoch: 7\n",
      "batch_idx: 500\n",
      "total_train_loss: 20629854267375.617\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 16930859296751.615\n",
      "\n",
      "epoch: 7\n",
      "batch_idx: 1000\n",
      "total_train_loss: 20307250845319.168\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 15955321109348.352\n",
      "\n",
      "epoch: 7\n",
      "batch_idx: 1500\n",
      "total_train_loss: 21942747141242.88\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 16734134837706.752\n",
      "\n",
      "epoch: 7\n",
      "batch_idx: 2000\n",
      "total_train_loss: 22713844879065.09\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 16815279482863.615\n",
      "\n",
      "epoch: 7\n",
      "batch_idx: 2500\n",
      "total_train_loss: 25267705426739.2\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 18714111988727.81\n",
      "\n",
      "epoch: 7\n",
      "batch_idx: 3000\n",
      "total_train_loss: 28018172640624.64\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 21289356939493.375\n",
      "\n",
      "epoch: 8\n",
      "batch_idx: 500\n",
      "total_train_loss: 35204717869006.85\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 26623064800231.426\n",
      "\n",
      "epoch: 8\n",
      "batch_idx: 1000\n",
      "total_train_loss: 50674973039132.67\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 39689780841152.516\n",
      "\n",
      "epoch: 8\n",
      "batch_idx: 1500\n",
      "total_train_loss: 60782197933604.87\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 46626890416390.14\n",
      "\n",
      "epoch: 8\n",
      "batch_idx: 2000\n",
      "total_train_loss: 59531051260706.81\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 42857636625907.71\n",
      "\n",
      "epoch: 8\n",
      "batch_idx: 2500\n",
      "total_train_loss: 62039949804830.72\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 42122947309076.48\n",
      "\n",
      "epoch: 8\n",
      "batch_idx: 3000\n",
      "total_train_loss: 73182288870899.72\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 50146539036737.54\n",
      "\n",
      "epoch: 9\n",
      "batch_idx: 500\n",
      "total_train_loss: 98504889411305.47\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 69707077546672.125\n",
      "\n",
      "epoch: 9\n",
      "batch_idx: 1000\n",
      "total_train_loss: 139988568247369.73\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 102992482165325.83\n",
      "\n",
      "epoch: 9\n",
      "batch_idx: 1500\n",
      "total_train_loss: 176323773534830.6\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 131983092872118.27\n",
      "\n",
      "epoch: 9\n",
      "batch_idx: 2000\n",
      "total_train_loss: 190558896555819.0\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 135294182934708.22\n",
      "\n",
      "epoch: 9\n",
      "batch_idx: 2500\n",
      "total_train_loss: 185523703960829.94\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 124071993279512.58\n",
      "\n",
      "epoch: 9\n",
      "batch_idx: 3000\n",
      "total_train_loss: 209928770470019.06\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 143114278461767.7\n",
      "\n",
      "epoch: 10\n",
      "batch_idx: 500\n",
      "total_train_loss: 301196560765550.56\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 211845846788997.12\n",
      "\n",
      "epoch: 10\n",
      "batch_idx: 1000\n",
      "total_train_loss: 388071893510389.75\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 267046897133289.47\n",
      "\n",
      "epoch: 10\n",
      "batch_idx: 1500\n",
      "total_train_loss: 488742428275114.0\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 330437964796526.56\n",
      "\n",
      "epoch: 10\n",
      "batch_idx: 2000\n",
      "total_train_loss: 559025158484918.25\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 367567605185118.2\n",
      "\n",
      "epoch: 10\n",
      "batch_idx: 2500\n",
      "total_train_loss: 587700940958597.1\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 365245378410315.75\n",
      "\n",
      "epoch: 10\n",
      "batch_idx: 3000\n",
      "total_train_loss: 644561968161620.0\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 389274176659128.3\n",
      "\n",
      "epoch: 11\n",
      "batch_idx: 500\n",
      "total_train_loss: 771946882015428.6\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 474192311731879.94\n",
      "\n",
      "epoch: 11\n",
      "batch_idx: 1000\n",
      "total_train_loss: 814625431675207.6\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 486720116751335.44\n",
      "\n",
      "epoch: 11\n",
      "batch_idx: 1500\n",
      "total_train_loss: 897088789665546.2\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 543969283839885.3\n",
      "\n",
      "epoch: 11\n",
      "batch_idx: 2000\n",
      "total_train_loss: 1034940262264078.4\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 662498469184798.8\n",
      "\n",
      "epoch: 11\n",
      "batch_idx: 2500\n",
      "total_train_loss: 1181276648645853.2\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 780942242595471.4\n",
      "\n",
      "epoch: 11\n",
      "batch_idx: 3000\n",
      "total_train_loss: 1311791858749276.2\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 872873992623489.0\n",
      "\n",
      "epoch: 12\n",
      "batch_idx: 500\n",
      "total_train_loss: 1433153212839362.5\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 901817567314706.4\n",
      "\n",
      "epoch: 12\n",
      "batch_idx: 1000\n",
      "total_train_loss: 1601918073193889.8\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 980947144980561.9\n",
      "\n",
      "epoch: 12\n",
      "batch_idx: 1500\n",
      "total_train_loss: 1723394176998965.2\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1021536310222913.5\n",
      "\n",
      "epoch: 12\n",
      "batch_idx: 2000\n",
      "total_train_loss: 1854055290094223.2\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1096164598014279.6\n",
      "\n",
      "epoch: 12\n",
      "batch_idx: 2500\n",
      "total_train_loss: 2180554174893129.8\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1332348110172586.0\n",
      "\n",
      "epoch: 12\n",
      "batch_idx: 3000\n",
      "total_train_loss: 2338906110913675.5\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1331496263136313.2\n",
      "\n",
      "epoch: 13\n",
      "batch_idx: 500\n",
      "total_train_loss: 2806852355188850.5\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1523528079635382.2\n",
      "\n",
      "epoch: 13\n",
      "batch_idx: 1000\n",
      "total_train_loss: 3412280238468497.5\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1928224894034641.0\n",
      "\n",
      "epoch: 13\n",
      "batch_idx: 1500\n",
      "total_train_loss: 4210207552006258.5\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2511366187988812.0\n",
      "\n",
      "epoch: 13\n",
      "batch_idx: 2000\n",
      "total_train_loss: 4679002403972841.0\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2746614359751917.5\n",
      "\n",
      "epoch: 13\n",
      "batch_idx: 2500\n",
      "total_train_loss: 6008655918504346.0\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3735910772308443.0\n",
      "\n",
      "epoch: 13\n",
      "batch_idx: 3000\n",
      "total_train_loss: 7056205119599149.0\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 4378097490210062.5\n",
      "\n",
      "epoch: 14\n",
      "batch_idx: 500\n",
      "total_train_loss: 6667953591229088.0\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3531095010617327.5\n",
      "\n",
      "epoch: 14\n",
      "batch_idx: 1000\n",
      "total_train_loss: 7104032831730352.0\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3774493625853935.5\n",
      "\n",
      "epoch: 14\n",
      "batch_idx: 1500\n",
      "total_train_loss: 9691580489957114.0\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 5849673411223093.0\n",
      "\n",
      "epoch: 14\n",
      "batch_idx: 2000\n",
      "total_train_loss: 1.1177072691809092e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 6630754425285640.0\n",
      "\n",
      "epoch: 14\n",
      "batch_idx: 2500\n",
      "total_train_loss: 1.2462265166064518e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 7197262601271640.0\n",
      "\n",
      "epoch: 14\n",
      "batch_idx: 3000\n",
      "total_train_loss: 1.4986430240631292e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 8661426084652253.0\n",
      "\n",
      "epoch: 15\n",
      "batch_idx: 500\n",
      "total_train_loss: 1.4452024045006226e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 7077662107268809.0\n",
      "\n",
      "epoch: 15\n",
      "batch_idx: 1000\n",
      "total_train_loss: 1.7009936941065438e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 8790367852277268.0\n",
      "\n",
      "epoch: 15\n",
      "batch_idx: 1500\n",
      "total_train_loss: 1.895482428662114e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.0000234617791054e+16\n",
      "\n",
      "epoch: 15\n",
      "batch_idx: 2000\n",
      "total_train_loss: 2.1175816989234428e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.1406587163836416e+16\n",
      "\n",
      "epoch: 15\n",
      "batch_idx: 2500\n",
      "total_train_loss: 2.5775074272102844e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.4800238531414328e+16\n",
      "\n",
      "epoch: 15\n",
      "batch_idx: 3000\n",
      "total_train_loss: 2.5948520809750332e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.3962327213392004e+16\n",
      "\n",
      "epoch: 16\n",
      "batch_idx: 500\n",
      "total_train_loss: 2.685978327361834e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.434725037510964e+16\n",
      "\n",
      "epoch: 16\n",
      "batch_idx: 1000\n",
      "total_train_loss: 2.7131625248569424e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.4020263670072738e+16\n",
      "\n",
      "epoch: 16\n",
      "batch_idx: 1500\n",
      "total_train_loss: 2.7730689705082092e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.4378046317119144e+16\n",
      "\n",
      "epoch: 16\n",
      "batch_idx: 2000\n",
      "total_train_loss: 2.9134702105983976e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.6207522569966846e+16\n",
      "\n",
      "epoch: 16\n",
      "batch_idx: 2500\n",
      "total_train_loss: 2.6498102763362188e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.4731272933724914e+16\n",
      "\n",
      "epoch: 16\n",
      "batch_idx: 3000\n",
      "total_train_loss: 2.5002154482955452e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.3952483444702314e+16\n",
      "\n",
      "epoch: 17\n",
      "batch_idx: 500\n",
      "total_train_loss: 2.2861693355592516e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.1468084014187806e+16\n",
      "\n",
      "epoch: 17\n",
      "batch_idx: 1000\n",
      "total_train_loss: 2.449318097268087e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.3068796874192846e+16\n",
      "\n",
      "epoch: 17\n",
      "batch_idx: 1500\n",
      "total_train_loss: 2.8332464990333172e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.6858518352228778e+16\n",
      "\n",
      "epoch: 17\n",
      "batch_idx: 2000\n",
      "total_train_loss: 2.8279412600810244e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.647004615595524e+16\n",
      "\n",
      "epoch: 17\n",
      "batch_idx: 2500\n",
      "total_train_loss: 2.692013404523279e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.4539517345631568e+16\n",
      "\n",
      "epoch: 17\n",
      "batch_idx: 3000\n",
      "total_train_loss: 2.9274323860620052e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.6208414312551678e+16\n",
      "\n",
      "epoch: 18\n",
      "batch_idx: 500\n",
      "total_train_loss: 2.9230277390451604e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.6078826277976408e+16\n",
      "\n",
      "epoch: 18\n",
      "batch_idx: 1000\n",
      "total_train_loss: 3.075233792955672e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.8296455563782916e+16\n",
      "\n",
      "epoch: 18\n",
      "batch_idx: 1500\n",
      "total_train_loss: 3.757513872618029e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.5608816032240108e+16\n",
      "\n",
      "epoch: 18\n",
      "batch_idx: 2000\n",
      "total_train_loss: 4.4180744650142776e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.293566495654255e+16\n",
      "\n",
      "epoch: 18\n",
      "batch_idx: 2500\n",
      "total_train_loss: 3.744146017119476e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.6371917103094564e+16\n",
      "\n",
      "epoch: 18\n",
      "batch_idx: 3000\n",
      "total_train_loss: 3.3234232507226916e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.163439349713915e+16\n",
      "\n",
      "epoch: 19\n",
      "batch_idx: 500\n",
      "total_train_loss: 3.208928939399866e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 1.967838926868788e+16\n",
      "\n",
      "epoch: 19\n",
      "batch_idx: 1000\n",
      "total_train_loss: 3.4773829848773164e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.185186690701931e+16\n",
      "\n",
      "epoch: 19\n",
      "batch_idx: 1500\n",
      "total_train_loss: 3.636880503834516e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.2898275292859596e+16\n",
      "\n",
      "epoch: 19\n",
      "batch_idx: 2000\n",
      "total_train_loss: 3.907573014540688e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.5060896928903988e+16\n",
      "\n",
      "epoch: 19\n",
      "batch_idx: 2500\n",
      "total_train_loss: 4.485348401357835e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.9353885355229576e+16\n",
      "\n",
      "epoch: 19\n",
      "batch_idx: 3000\n",
      "total_train_loss: 5.0071364065510296e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.363985870624391e+16\n",
      "\n",
      "epoch: 20\n",
      "batch_idx: 500\n",
      "total_train_loss: 5.322926204861704e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.3788911620651484e+16\n",
      "\n",
      "epoch: 20\n",
      "batch_idx: 1000\n",
      "total_train_loss: 5.607581251808736e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.62768175462183e+16\n",
      "\n",
      "epoch: 20\n",
      "batch_idx: 1500\n",
      "total_train_loss: 5.316568161191985e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.4470436537259524e+16\n",
      "\n",
      "epoch: 20\n",
      "batch_idx: 2000\n",
      "total_train_loss: 4.685245813354817e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.8467092449623276e+16\n",
      "\n",
      "epoch: 20\n",
      "batch_idx: 2500\n",
      "total_train_loss: 4.5934058361747144e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.6974277841780212e+16\n",
      "\n",
      "epoch: 20\n",
      "batch_idx: 3000\n",
      "total_train_loss: 4.7547778686974424e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.762526676927434e+16\n",
      "\n",
      "epoch: 21\n",
      "batch_idx: 500\n",
      "total_train_loss: 4.888205089927751e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.7847060783110292e+16\n",
      "\n",
      "epoch: 21\n",
      "batch_idx: 1000\n",
      "total_train_loss: 5.088300449665961e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.9005152264405384e+16\n",
      "\n",
      "epoch: 21\n",
      "batch_idx: 1500\n",
      "total_train_loss: 5.0243411934172936e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.7500884284453944e+16\n",
      "\n",
      "epoch: 21\n",
      "batch_idx: 2000\n",
      "total_train_loss: 5.047380106662642e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.675518589065442e+16\n",
      "\n",
      "epoch: 21\n",
      "batch_idx: 2500\n",
      "total_train_loss: 5.5835844156864856e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.0403360459388356e+16\n",
      "\n",
      "epoch: 21\n",
      "batch_idx: 3000\n",
      "total_train_loss: 5.893273036859533e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.1935481078916056e+16\n",
      "\n",
      "epoch: 22\n",
      "batch_idx: 500\n",
      "total_train_loss: 6.265500155420606e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.42362020862262e+16\n",
      "\n",
      "epoch: 22\n",
      "batch_idx: 1000\n",
      "total_train_loss: 6.7269670152542616e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.792519717948752e+16\n",
      "\n",
      "epoch: 22\n",
      "batch_idx: 1500\n",
      "total_train_loss: 7.240408766408451e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 4.2377953301835544e+16\n",
      "\n",
      "epoch: 22\n",
      "batch_idx: 2000\n",
      "total_train_loss: 7.304628667042445e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 4.229652772699505e+16\n",
      "\n",
      "epoch: 22\n",
      "batch_idx: 2500\n",
      "total_train_loss: 7.02888006972531e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.835446738821736e+16\n",
      "\n",
      "epoch: 22\n",
      "batch_idx: 3000\n",
      "total_train_loss: 6.9023445478642024e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.657813472637564e+16\n",
      "\n",
      "epoch: 23\n",
      "batch_idx: 500\n",
      "total_train_loss: 6.646745369568451e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.3881220883138216e+16\n",
      "\n",
      "epoch: 23\n",
      "batch_idx: 1000\n",
      "total_train_loss: 6.611450198490297e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.3374148776360412e+16\n",
      "\n",
      "epoch: 23\n",
      "batch_idx: 1500\n",
      "total_train_loss: 6.413765815207723e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.1692513615045196e+16\n",
      "\n",
      "epoch: 23\n",
      "batch_idx: 2000\n",
      "total_train_loss: 6.100078025431318e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.860143463718558e+16\n",
      "\n",
      "epoch: 23\n",
      "batch_idx: 2500\n",
      "total_train_loss: 5.946672929609403e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.6757022298411108e+16\n",
      "\n",
      "epoch: 23\n",
      "batch_idx: 3000\n",
      "total_train_loss: 6.213866001035533e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.9937380301891176e+16\n",
      "\n",
      "epoch: 24\n",
      "batch_idx: 500\n",
      "total_train_loss: 6.3143507767198744e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.0932591852929416e+16\n",
      "\n",
      "epoch: 24\n",
      "batch_idx: 1000\n",
      "total_train_loss: 6.3709021231980216e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.077175676519213e+16\n",
      "\n",
      "epoch: 24\n",
      "batch_idx: 1500\n",
      "total_train_loss: 6.511775209683838e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.2366343784209844e+16\n",
      "\n",
      "epoch: 24\n",
      "batch_idx: 2000\n",
      "total_train_loss: 6.4894431527071384e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 3.349122437505404e+16\n",
      "\n",
      "epoch: 24\n",
      "batch_idx: 2500\n",
      "total_train_loss: 5.9694705962597224e+16\n",
      "best_train_loss: 348782.023609375\n",
      "recon_error: 2.857545707725154e+16\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 17\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m n_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m eval_every) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    132\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    135\u001b[0m         group,\n\u001b[1;32m    136\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    141\u001b[0m         state_steps)\n\u001b[0;32m--> 143\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py:283\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 283\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py:427\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m grad_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m found_inf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m differentiable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_foreach ops don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 427\u001b[0m grouped_tensors \u001b[38;5;241m=\u001b[39m \u001b[43m_group_tensors_by_device_and_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (device_params, device_grads, device_exp_avgs, device_exp_avg_sqs,\n\u001b[1;32m    429\u001b[0m      device_max_exp_avg_sqs, device_state_steps) \u001b[38;5;129;01min\u001b[39;00m grouped_tensors\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m maximize:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_foreach_utils.py:30\u001b[0m, in \u001b[0;36m_group_tensors_by_device_and_dtype\u001b[0;34m(tensorlistlist, with_indices)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tensorlistlist[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     29\u001b[0m     key \u001b[38;5;241m=\u001b[39m (t\u001b[38;5;241m.\u001b[39mdevice, t\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensorlistlist\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# a tensorlist may be empty/None\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tensorlistlist[j]:\n\u001b[1;32m     33\u001b[0m             per_device_and_dtype_tensors[key][j]\u001b[38;5;241m.\u001b[39mappend(tensorlistlist[j][i])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    total_recon_error = 0\n",
    "    n_train = 0\n",
    "    for (batch_idx, train_tensors) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        imgs = train_tensors[0].unsqueeze(1).to(device)\n",
    "        out = model(imgs)\n",
    "        recon_error = criterion(out[\"x_recon\"], imgs) / train_data_variance\n",
    "        total_recon_error += recon_error.item()\n",
    "        loss = recon_error + beta * out[\"commitment_loss\"]\n",
    "        if not use_ema:\n",
    "            loss += out[\"dictionary_loss\"]\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        n_train += 1\n",
    "\n",
    "        if ((batch_idx + 1) % eval_every) == 0:\n",
    "            print(f\"epoch: {epoch}\\nbatch_idx: {batch_idx + 1}\", flush=True)\n",
    "            total_train_loss /= n_train\n",
    "            if total_train_loss < best_train_loss:\n",
    "                best_train_loss = total_train_loss\n",
    "\n",
    "            print(f\"total_train_loss: {total_train_loss}\")\n",
    "            print(f\"best_train_loss: {best_train_loss}\")\n",
    "            print(f\"recon_error: {total_recon_error / n_train}\\n\")\n",
    "\n",
    "            total_train_loss = 0\n",
    "            total_recon_error = 0\n",
    "            n_train = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85800e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE definition\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=64):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, input_dim),\n",
    "            nn.Sigmoid()  # because you scaled inputs to [0,1]\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = self.fc_mu(h), self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "\n",
    "# VAE loss (Reconstruction + KL Divergence)\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')  # or L1 if you prefer\n",
    "    # KL divergence\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44355fe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---- Training loop ----\n",
    "output_units = x_train_scaled.shape[1]\n",
    "latent_dim = 64\n",
    "model = VAE(input_dim=output_units, latent_dim=latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "num_epochs = 200\n",
    "train_losses, val_losses = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, _ in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = vae_loss(recon_batch, data, mu, logvar)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58375d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Extract embeddings (mean of latent space) ----\n",
    "with torch.no_grad():\n",
    "    mu_train, _ = model.fc_mu(model.encoder(x_train_tensor.to(device))), model.fc_logvar(model.encoder(x_train_tensor.to(device)))\n",
    "    mu_test, _  = model.fc_mu(model.encoder(x_test_tensor.to(device))),  model.fc_logvar(model.encoder(x_test_tensor.to(device)))\n",
    "train, test = mu_train.cpu().numpy(), mu_test.cpu().numpy()\n",
    "\n",
    "#---- Acquire Patient Data ----\n",
    "recon_train_df, recon_test_df = train_df.drop('embedding', axis=1), test_df.drop('embedding', axis=1)\n",
    "recon_train_df[\"embedding\"], recon_test_df[\"embedding\"] = [row.tolist() for row in train], [row.tolist() for row in test]\n",
    "\n",
    "#---- Save Low Dimensional Embeddings ----\n",
    "reduced_train_df.to_csv(f'{savepath}/train_nlst_{dim}.csv', index=False); reduced_test_df.to_csv(f'{savepath}/test_nlst_{dim}.csv', index=False) \n",
    "print(f\"{dim} dimensional NLST embeddings saved to {savepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
